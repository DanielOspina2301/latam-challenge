{
 "cells": [
  {
   "cell_type": "raw",
   "source": [
    "En este archivo puedes escribir lo que estimes conveniente. Te recomendamos detallar tu solución y todas las suposiciones que estás considerando. Aquí puedes ejecutar las funciones que definiste en los otros archivos de la carpeta src, medir el tiempo, memoria, etc."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": ""
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import gc\n",
    "import heapq\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Third-party imports\n",
    "import emoji\n",
    "import memray\n",
    "import orjson\n",
    "import pandas as pd\n",
    "import snakeviz\n",
    "import ujson"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "file_path = \"farmers-protest-tweets-2021-2-4.json\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### First, I tried to load the file in a pandas to evaluate if the file was correct."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_json(file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### As the load fails I checked the file to find what happened. I used an online tool to prove some random objects and I saw that each object was correct. So I assumed that the problem was that the file contained a lot of objects but them were not in a list. In the next cell I read the file and fix it adding each element in a list and convert to a json. I save the file again in a new file."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# List to save the elements\n",
    "json_list = []\n",
    "\n",
    "# Read the file and add each JSON to the list\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            # Load each line as JSON object and append to the list\n",
    "            object_ = json.loads(line)\n",
    "            json_list.append(object_)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f'Error decoding the line: {line.strip()} - {e}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Convert the list to JSON\n",
    "valid_json = json.dumps(json_list, indent=4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open('fixed_data.json', 'w') as end_file:\n",
    "    end_file.write(valid_json)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "file_path_test = \"fixed_data.json\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_json(file_path_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(min(df[\"date\"]), max(df[\"date\"]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### After processing the file like I explain before. I tried to load in a pandas again and this time it was correct. So I was able to visualize the file structure, fields, types and validate the date range of the file as well as the quantity of rows."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### After understanding the problem with the file I solved the first exercise knowing the problems that the file had and knowing that I had to correct it during the execution of the function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### I defined a function to process the file in a way to optimize the execution time. I proved three packages to use the one that had better performance.\n",
    "\n",
    "### I proved with json, ujson and orjson\n",
    "\n",
    "### Finally, after checked the execution time of all of them. I decided to use orjson"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def process_json(file_path):\n",
    "    tweets_per_date = defaultdict(lambda: defaultdict(int))\n",
    "    # Open the file in read mode\n",
    "    with open(file_path, 'r') as f:\n",
    "        # Read the file line by line\n",
    "        for line in f:\n",
    "            # Fix and convert each line in a JSON object\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                # Ignore empty lines\n",
    "                continue\n",
    "\n",
    "            # Validate the object to process correctly\n",
    "            if line.startswith('{') and line.endswith('}'):\n",
    "                try:\n",
    "                    tweet = orjson.loads(line)\n",
    "                except orjson.JSONDecodeError as e:\n",
    "                    print(f'Error decoding the line: {line.strip()} - {e}')\n",
    "\n",
    "                # Obtain date and username of each tweet\n",
    "                tweet_date = datetime.strptime(tweet['date'], '%Y-%m-%dT%H:%M:%S+00:00').date()\n",
    "                username = tweet['user']['username']\n",
    "\n",
    "                # Add to the count of that date and user\n",
    "                tweets_per_date[tweet_date][username] += 1\n",
    "\n",
    "    return tweets_per_date"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Now I create the function to return the top 10 of dates with more tweets and the username with more publications. First, I implemented a loop to extract username and date of each tweet and append to a defaultdict. Here I add a new parameter to the function called \"version\" to proved two versions that I implemented. The first one has a loop over the items of the defaultdict and for each date calculate the total tweets and the user with more tweets, then make a validation with the maximum found. The second one sorted the dates by the total tweets and then just for the top 10 calculate the user with more tweets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def q1_time(file_path: str, version: int) -> List[Tuple[datetime.date, str]]:\n",
    "    tweets_per_date = process_json(file_path)\n",
    "\n",
    "    if version == 1:\n",
    "        top_dates = []\n",
    "        for date, users in tweets_per_date.items():\n",
    "            # Calculate total tweets and find user with more tweets\n",
    "            total_tweets = 0\n",
    "            top_user = None\n",
    "            max_tweets = 0\n",
    "\n",
    "            for user, count in users.items():\n",
    "                total_tweets += count\n",
    "                if count > max_tweets:\n",
    "                    max_tweets = count\n",
    "                    top_user = user\n",
    "\n",
    "            # Add date, total tweets and user to the list\n",
    "            top_dates.append((date, total_tweets, top_user))\n",
    "\n",
    "        # Sort list by total tweets and save just top 10\n",
    "        top_dates_sorted = sorted(top_dates, key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "        # Return date and user in a list of tuples\n",
    "        return [(date, top_user) for date, _, top_user in top_dates_sorted]\n",
    "\n",
    "    else:\n",
    "        # List to save dates and total tweets per date\n",
    "        date_tweet_sums = []\n",
    "        # Dict to save users with more tweets per date\n",
    "        top_users_by_date = {}\n",
    "\n",
    "        # Calculate total tweets and find user with more tweets\n",
    "        for date, users in tweets_per_date.items():\n",
    "            total_tweets = sum(users.values())\n",
    "            top_user = max(users.items(), key=lambda item: item[1])[0]\n",
    "            date_tweet_sums.append((date, total_tweets))\n",
    "            top_users_by_date[date] = top_user\n",
    "\n",
    "        # Sort list by total tweets and save just top 10\n",
    "        top_10_dates = sorted(date_tweet_sums, key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "        # Use the date list and users dict to return the list\n",
    "        return [(tweet_date, top_users_by_date[tweet_date]) for tweet_date, _ in top_10_dates]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Now, for the function optimized by memory use I implemented a function that read the file line by line with a generator. The \"tweet_generator\" function process the file line by line and use yield to return each tweet one by one. With the use of yield avoid that the data are in memory and allow to procees when is necessary. In addition I used heap to implement a priority queue, where the element with the minimum or maximum value can be accessed efficiently, to obtain the k largest or smallest elements from a list efficiently"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def tweet_generator(file_path):\n",
    "    # Open file in read mode\n",
    "    with open(file_path, 'r') as f:\n",
    "        # Read each line\n",
    "        for line in f:\n",
    "            # Delete line breaks and spaces\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            # Validate JSON format\n",
    "            if line.startswith('{') and line.endswith('}'):\n",
    "                # Convert to object\n",
    "                try:\n",
    "                    tweet = json.loads(line)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f'Error decoding the line: {line.strip()} - {e}')\n",
    "\n",
    "                # Save the date and username\n",
    "                tweet_date = datetime.strptime(tweet['date'], '%Y-%m-%dT%H:%M:%S+00:00').date()\n",
    "                username = tweet['user']['username']\n",
    "\n",
    "                # Use of yield to return data one by one\n",
    "                yield tweet_date, username\n",
    "\n",
    "def process_tweets(file_path):\n",
    "    # Dict to count tweets by date and user\n",
    "    tweets_per_date = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    # use generator to process each tweet\n",
    "    for tweet_date, username in tweet_generator(file_path):\n",
    "        # Add to dict\n",
    "        tweets_per_date[tweet_date][username] += 1\n",
    "\n",
    "    return tweets_per_date\n",
    "\n",
    "def q1_memory(file_path):\n",
    "    top_dates_heap = []\n",
    "    tweets = process_tweets(file_path)\n",
    "\n",
    "    # Process file in an incremental way and free memory whenever possible\n",
    "    for date, user_data in tweets.items():\n",
    "        # Total tweets by user\n",
    "        total_tweets = sum(user_data.values())\n",
    "        top_user = max(user_data, key=user_data.get)\n",
    "        top_user_count = user_data[top_user]\n",
    "\n",
    "        # Use heap\n",
    "        heapq.heappush(top_dates_heap, (total_tweets, date, top_user, top_user_count))\n",
    "\n",
    "        # Limit the heap to 10 elements\n",
    "        if len(top_dates_heap) > 10:\n",
    "            heapq.heappop(top_dates_heap)\n",
    "\n",
    "        # Free memory manually\n",
    "        del user_data\n",
    "        gc.collect()\n",
    "\n",
    "    # Convert the heap to sorted list\n",
    "    top_dates_sorted = sorted(top_dates_heap, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    return [(date, top_user) for _, date, top_user, _ in top_dates_sorted]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "q1_time(file_path=file_path, version=2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "q1_memory(file_path=file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Use of snakeviz. SnakeViz is a browser based graphical viewer for the output of Python’s cProfile module. With that package we can see each function and their times"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext snakeviz"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Here I use snakeviz to see the time profile of each function. I run the time function twice, because I was proving the two versions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%snakeviz q1_time(file_path=file_path, version=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%snakeviz q1_time(file_path=file_path, version=2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%snakeviz q1_memory(file_path=file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Use of memray to see the memory used profile"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext memray"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%memray_flamegraph\n",
    "def q1_time(file_path: str, version: int) -> List[Tuple[datetime.date, str]]:\n",
    "    tweets_per_date = process_json(file_path)\n",
    "\n",
    "    if version == 1:\n",
    "        top_dates = []\n",
    "        for date, users in tweets_per_date.items():\n",
    "            # Calculate total tweets and find user with more tweets\n",
    "            total_tweets = 0\n",
    "            top_user = None\n",
    "            max_tweets = 0\n",
    "\n",
    "            for user, count in users.items():\n",
    "                total_tweets += count\n",
    "                if count > max_tweets:\n",
    "                    max_tweets = count\n",
    "                    top_user = user\n",
    "\n",
    "            # Add date, total tweets and user to the list\n",
    "            top_dates.append((date, total_tweets, top_user))\n",
    "\n",
    "        # Sort list by total tweets and save just top 10\n",
    "        top_dates_sorted = sorted(top_dates, key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "        # Return date and user in a list of tuples\n",
    "        return [(date, top_user) for date, _, top_user in top_dates_sorted]\n",
    "\n",
    "    else:\n",
    "        # List to save dates and total tweets per date\n",
    "        date_tweet_sums = []\n",
    "        # Dict to save users with more tweets per date\n",
    "        top_users_by_date = {}\n",
    "\n",
    "        # Calculate total tweets and find user with more tweets\n",
    "        for date, users in tweets_per_date.items():\n",
    "            total_tweets = sum(users.values())\n",
    "            top_user = max(users.items(), key=lambda item: item[1])[0]\n",
    "            date_tweet_sums.append((date, total_tweets))\n",
    "            top_users_by_date[date] = top_user\n",
    "\n",
    "        # Sort list by total tweets and save just top 10\n",
    "        top_10_dates = sorted(date_tweet_sums, key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "        # Use the date list and users dict to return the list\n",
    "        return [(tweet_date, top_users_by_date[tweet_date]) for tweet_date, _ in top_10_dates]\n",
    "\n",
    "q1_time(file_path=file_path, version=2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%memray_flamegraph\n",
    "def tweet_generator(file_path):\n",
    "    # Open file in read mode\n",
    "    with open(file_path, 'r') as f:\n",
    "        # Read each line\n",
    "        for line in f:\n",
    "            # Delete line breaks and spaces\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            # Validate JSON format\n",
    "            if line.startswith('{') and line.endswith('}'):\n",
    "                try:\n",
    "                    # Convert to object\n",
    "                    tweet = json.loads(line)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f'Error decoding the line: {line.strip()} - {e}')\n",
    "\n",
    "                # Save the date and username\n",
    "                tweet_date = datetime.strptime(tweet['date'], '%Y-%m-%dT%H:%M:%S+00:00').date()\n",
    "                username = tweet['user']['username']\n",
    "\n",
    "                # Use of yield to return data one by one\n",
    "                yield tweet_date, username\n",
    "\n",
    "def process_tweets(file_path):\n",
    "    # Dict to count tweets by date and user\n",
    "    tweets_per_date = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    # use generator to process each tweet\n",
    "    for tweet_date, username in tweet_generator(file_path):\n",
    "        # Add to dict\n",
    "        tweets_per_date[tweet_date][username] += 1\n",
    "\n",
    "    return tweets_per_date\n",
    "\n",
    "def q1_memory(file_path):\n",
    "    top_dates_heap = []\n",
    "    tweets = process_tweets(file_path)\n",
    "\n",
    "    # Process file in an incremental way and free memory whenever possible\n",
    "    for date, user_data in tweets.items():\n",
    "        # Total tweets by user\n",
    "        total_tweets = sum(user_data.values())\n",
    "        top_user = max(user_data, key=user_data.get)\n",
    "        top_user_count = user_data[top_user]\n",
    "\n",
    "        # Use heap\n",
    "        heapq.heappush(top_dates_heap, (total_tweets, date, top_user, top_user_count))\n",
    "\n",
    "        # Limit the heap to 10 elements\n",
    "        if len(top_dates_heap) > 10:\n",
    "            heapq.heappop(top_dates_heap)\n",
    "\n",
    "        # Free memory manually\n",
    "        del user_data\n",
    "        gc.collect()\n",
    "\n",
    "    # Convert the heap to sorted list\n",
    "    top_dates_sorted = sorted(top_dates_heap, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    return [(date, top_user) for _, date, top_user, _ in top_dates_sorted]\n",
    "\n",
    "q1_memory(file_path=file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### After use the packages, we can see that in terms of time the function q1_time is more efficient than q1_memory. However, in terms of memory we can see in the profiling that both functions has the same use of memory that is because in both cases I read the file line by line so just save in memory the last object that I processed. I try to load all the file in memory in q1_time but the orjson.loads function took a lot of time trying to convert the entire file to an object."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### For the second exercise I implemented a function to extract emojis from a text. Then I process the file line by line and extract emojis from the text and save in a Counter.\n",
    "\n",
    "### For this exercise I assumed that I should to find the most used emojis regardless of whether they were in the main tweet or in a quoted tweet.\n",
    "\n",
    "### I used the emoji package because even the use of regular expressions can be more efficient, that way has a complexity in maintain uploaded the list of expressions. The package emoji is uploaded with each new version of unicode"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def extract_emojis(text):\n",
    "    \"\"\"Extract emojis from text.\"\"\"\n",
    "    # Use emoji package to find emojis\n",
    "    return [char for char in text if char in emoji.EMOJI_DATA]\n",
    "\n",
    "def q2_time(file_path):\n",
    "    emoji_counter = Counter()\n",
    "    with open(file_path, 'r') as f:\n",
    "            # Read the file line by line\n",
    "            for line in f:\n",
    "                # Fix and convert each line in a JSON object\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    # Ignore empty lines\n",
    "                    continue\n",
    "\n",
    "                # Validate the object to process correctly\n",
    "                if line.startswith('{') and line.endswith('}'):\n",
    "                    try:\n",
    "                        tweet = orjson.loads(line)\n",
    "                    except orjson.JSONDecodeError as e:\n",
    "                        print(f'Error decoding the line: {line.strip()} - {e}')\n",
    "\n",
    "                # Extract emojis from 'content'\n",
    "                content_emojis = extract_emojis(tweet.get('content', ''))\n",
    "                emoji_counter.update(content_emojis)\n",
    "\n",
    "                # Check if quotedTweet exists and extract emojis from their 'content'\n",
    "                quoted_tweet = tweet.get('quotedTweet')\n",
    "                if quoted_tweet:\n",
    "                    quoted_content_emojis = extract_emojis(quoted_tweet.get('content', ''))\n",
    "                    emoji_counter.update(quoted_content_emojis)\n",
    "\n",
    "            # Obtain top 10 of emojis\n",
    "            top_10 = emoji_counter.most_common(10)\n",
    "\n",
    "            return top_10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "q2_time(file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### For the memory function I used the same approach. I extract emojis with the emoji package from the content of the main tweet and the quoted tweet. And I used generator to process the file in incrementally way"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def extract_emojis(text):\n",
    "    \"\"\"Extract emojis from text.\"\"\"\n",
    "    return [char for char in text if char in emoji.EMOJI_DATA]\n",
    "\n",
    "def tweet_generator(file_path):\n",
    "    \"\"\"Generator to process each tweet and their quotedTweet.\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "            # Read the file line by line\n",
    "            for line in f:\n",
    "                # Fix and convert each line in a JSON object\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    # Ignore empty lines\n",
    "                    continue\n",
    "\n",
    "                # Validate the object to process correctly\n",
    "                if line.startswith('{') and line.endswith('}'):\n",
    "                    try:\n",
    "                        tweet = json.loads(line)\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f'Error decoding the line: {line.strip()} - {e}')\n",
    "\n",
    "                # Process emojis from 'content'\n",
    "                yield extract_emojis(tweet.get('content', ''))\n",
    "\n",
    "                # Process emojis from 'content' in the 'quotedTweet' if it exists\n",
    "                quoted_tweet = tweet.get('quotedTweet')\n",
    "                if quoted_tweet:\n",
    "                    yield extract_emojis(quoted_tweet.get('content', ''))\n",
    "\n",
    "def q2_memory(file_path):\n",
    "    emoji_counter = Counter()\n",
    "\n",
    "    # Use generator to process tweets one by one\n",
    "    for emoji_list in tweet_generator(file_path):\n",
    "        for emoji_char in emoji_list:\n",
    "            emoji_counter[emoji_char] += 1\n",
    "\n",
    "    # Obtain top 10 of emojis\n",
    "    return emoji_counter.most_common(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "q2_memory(file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%snakeviz q2_time(file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%snakeviz q2_memory(file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%memray_flamegraph\n",
    "def extract_emojis(text):\n",
    "    \"\"\"Extract emojis from text.\"\"\"\n",
    "    # Use emoji package to find emojis\n",
    "    return [char for char in text if char in emoji.EMOJI_DATA]\n",
    "\n",
    "def q2_time(file_path):\n",
    "    emoji_counter = Counter()\n",
    "    with open(file_path, 'r') as f:\n",
    "            # Read the file line by line\n",
    "            for line in f:\n",
    "                # Fix and convert each line in a JSON object\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    # Ignore empty lines\n",
    "                    continue\n",
    "\n",
    "                # Validate the object to process correctly\n",
    "                if line.startswith('{') and line.endswith('}'):\n",
    "                    try:\n",
    "                        tweet = orjson.loads(line)\n",
    "                    except orjson.JSONDecodeError as e:\n",
    "                        print(f'Error decoding the line: {line.strip()} - {e}')\n",
    "\n",
    "                # Extract emojis from 'content'\n",
    "                content_emojis = extract_emojis(tweet.get('content', ''))\n",
    "                emoji_counter.update(content_emojis)\n",
    "\n",
    "                # Check if quotedTweet exists and extract emojis from their 'content'\n",
    "                quoted_tweet = tweet.get('quotedTweet')\n",
    "                if quoted_tweet:\n",
    "                    quoted_content_emojis = extract_emojis(quoted_tweet.get('content', ''))\n",
    "                    emoji_counter.update(quoted_content_emojis)\n",
    "\n",
    "            # Obtain top 10 of emojis\n",
    "            top_10 = emoji_counter.most_common(10)\n",
    "\n",
    "            return top_10\n",
    "\n",
    "q2_time(file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%memray_flamegraph\n",
    "def extract_emojis(text):\n",
    "    \"\"\"Extract emojis from text.\"\"\"\n",
    "    return [char for char in text if char in emoji.EMOJI_DATA]\n",
    "\n",
    "def tweet_generator(file_path):\n",
    "    \"\"\"Generator to process each tweet and their quotedTweet.\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "            # Read the file line by line\n",
    "            for line in f:\n",
    "                # Fix and convert each line in a JSON object\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    # Ignore empty lines\n",
    "                    continue\n",
    "\n",
    "                # Validate the object to process correctly\n",
    "                if line.startswith('{') and line.endswith('}'):\n",
    "                    try:\n",
    "                        tweet = json.loads(line)\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f'Error decoding the line: {line.strip()} - {e}')\n",
    "\n",
    "                # Process emojis from 'content'\n",
    "                yield extract_emojis(tweet.get('content', ''))\n",
    "\n",
    "                # Process emojis from 'content' in the 'quotedTweet' if it exists\n",
    "                quoted_tweet = tweet.get('quotedTweet')\n",
    "                if quoted_tweet:\n",
    "                    yield extract_emojis(quoted_tweet.get('content', ''))\n",
    "\n",
    "def q2_memory(file_path):\n",
    "    emoji_counter = Counter()\n",
    "\n",
    "    # Use generator to process tweets one by one\n",
    "    for emoji_list in tweet_generator(file_path):\n",
    "        for emoji_char in emoji_list:\n",
    "            emoji_counter[emoji_char] += 1  # Contar emojis uno por uno\n",
    "\n",
    "    # Obtain top 10 of emojis\n",
    "    return emoji_counter.most_common(10)\n",
    "\n",
    "q2_memory(file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### In the second exercise, after use snakeviz we can see that the function q2_time has a better performance in terms of execution time. For memory, in the profiling we can see that the function q2_memory has a better use of memory but the difference is 2 KiB. In large data sets there could be more difference"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### For exercise 3 like in exercise 2 I assumed that I should to find the most influential users regardless of whether they were in the main tweet or in a quoted tweet."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def q3_time(file_path):\n",
    "    mention_counter = Counter()\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "            # Read the file line by line\n",
    "            for line in f:\n",
    "                # Fix and convert each line in a JSON object\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    # Ignore empty lines\n",
    "                    continue\n",
    "\n",
    "                # Validate the object to process correctly\n",
    "                if line.startswith('{') and line.endswith('}'):\n",
    "                    try:\n",
    "                        tweet = orjson.loads(line)\n",
    "                    except orjson.JSONDecodeError as e:\n",
    "                        print(f'Error decoding the line: {line.strip()} - {e}')\n",
    "\n",
    "                # Process mentions in 'mentionedUsers' from main tweet\n",
    "                mentioned_users = tweet.get('mentionedUsers', [])\n",
    "                if mentioned_users is None:\n",
    "                    mentioned_users = []\n",
    "                mention_counter.update(user['username'] for user in mentioned_users)\n",
    "\n",
    "                # Process mentions in 'mentionedUsers' from 'quotedTweet' if it exists\n",
    "                quoted_tweet = tweet.get('quotedTweet')\n",
    "                if quoted_tweet:\n",
    "                    quoted_mentioned_users = quoted_tweet.get('mentionedUsers', [])\n",
    "                    if quoted_mentioned_users is None:\n",
    "                        quoted_mentioned_users = []\n",
    "                    mention_counter.update(user['username'] for user in quoted_mentioned_users)\n",
    "\n",
    "            # Obtain top 10 of more mentioned users\n",
    "            return mention_counter.most_common(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "q3_time(file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### As well as in the other exercises, for the memory implementation I used generators to maintain the memory with an efficient use."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def mention_generator(file_path):\n",
    "    \"\"\"Generator to process mentions of each tweet and their quotedTweet.\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "            # Read the file line by line\n",
    "            for line in f:\n",
    "                # Fix and convert each line in a JSON object\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    # Ignore empty lines\n",
    "                    continue\n",
    "\n",
    "                # Validate the object to process correctly\n",
    "                if line.startswith('{') and line.endswith('}'):\n",
    "                    try:\n",
    "                        tweet = json.loads(line)\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f'Error decoding the line: {line.strip()} - {e}')\n",
    "\n",
    "                # Process mentions in 'mentionedUsers' from main tweet\n",
    "                mentioned_users = tweet.get('mentionedUsers', [])\n",
    "                if mentioned_users is None:\n",
    "                    mentioned_users = []\n",
    "                for user in mentioned_users:\n",
    "                    yield user['username']\n",
    "\n",
    "                # Process mentions in 'mentionedUsers' from 'quotedTweet' if it exists\n",
    "                quoted_tweet = tweet.get('quotedTweet')\n",
    "                if quoted_tweet:\n",
    "                    quoted_mentioned_users = quoted_tweet.get('mentionedUsers', [])\n",
    "                    if quoted_mentioned_users is None:\n",
    "                        quoted_mentioned_users = []\n",
    "                    for user in quoted_mentioned_users:\n",
    "                        yield user['username']\n",
    "\n",
    "def q3_memory(file_path):\n",
    "    mention_counter = Counter()\n",
    "\n",
    "    # Use generator to process mentions\n",
    "    for username in mention_generator(file_path):\n",
    "        mention_counter[username] += 1\n",
    "\n",
    "    # Obtain top 10 of more mentioned users\n",
    "    return mention_counter.most_common(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "q3_memory(file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%snakeviz q3_time(file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%snakeviz q3_memory(file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%memray_flamegraph\n",
    "def q3_time(file_path):\n",
    "    mention_counter = Counter()\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "            # Read the file line by line\n",
    "            for line in f:\n",
    "                # Fix and convert each line in a JSON object\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    # Ignore empty lines\n",
    "                    continue\n",
    "\n",
    "                # Validate the object to process correctly\n",
    "                if line.startswith('{') and line.endswith('}'):\n",
    "                    try:\n",
    "                        tweet = orjson.loads(line)\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f'Error decoding the line: {line.strip()} - {e}')\n",
    "\n",
    "                # Process mentions in 'mentionedUsers' from main tweet\n",
    "                mentioned_users = tweet.get('mentionedUsers', [])\n",
    "                if mentioned_users is None:\n",
    "                    mentioned_users = []\n",
    "                mention_counter.update(user['username'] for user in mentioned_users)\n",
    "\n",
    "                # Process mentions in 'mentionedUsers' from 'quotedTweet' if it exists\n",
    "                quoted_tweet = tweet.get('quotedTweet')\n",
    "                if quoted_tweet:\n",
    "                    quoted_mentioned_users = quoted_tweet.get('mentionedUsers', [])\n",
    "                    if quoted_mentioned_users is None:\n",
    "                        quoted_mentioned_users = []\n",
    "                    mention_counter.update(user['username'] for user in quoted_mentioned_users)\n",
    "\n",
    "            # Obtain top 10 of more mentioned users\n",
    "            return mention_counter.most_common(10)\n",
    "\n",
    "q3_time(file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%memray_flamegraph\n",
    "def mention_generator(file_path):\n",
    "    \"\"\"Generator to process mentions of each tweet and their quotedTweet.\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "            # Read the file line by line\n",
    "            for line in f:\n",
    "                # Fix and convert each line in a JSON object\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    # Ignore empty lines\n",
    "                    continue\n",
    "\n",
    "                # Validate the object to process correctly\n",
    "                if line.startswith('{') and line.endswith('}'):\n",
    "                    try:\n",
    "                        tweet = json.loads(line)\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f'Error decoding the line: {line.strip()} - {e}')\n",
    "\n",
    "                # Process mentions in 'mentionedUsers' from main tweet\n",
    "                mentioned_users = tweet.get('mentionedUsers', [])\n",
    "                if mentioned_users is None:\n",
    "                    mentioned_users = []\n",
    "                for user in mentioned_users:\n",
    "                    yield user['username']\n",
    "\n",
    "                # Process mentions in 'mentionedUsers' from 'quotedTweet' if it exists\n",
    "                quoted_tweet = tweet.get('quotedTweet')\n",
    "                if quoted_tweet:\n",
    "                    quoted_mentioned_users = quoted_tweet.get('mentionedUsers', [])\n",
    "                    if quoted_mentioned_users is None:\n",
    "                        quoted_mentioned_users = []\n",
    "                    for user in quoted_mentioned_users:\n",
    "                        yield user['username']\n",
    "\n",
    "def q3_memory(file_path):\n",
    "    mention_counter = Counter()\n",
    "\n",
    "    # Use generator to process mentions\n",
    "    for username in mention_generator(file_path):\n",
    "        mention_counter[username] += 1\n",
    "\n",
    "    # Obtain top 10 of more mentioned users\n",
    "    return mention_counter.most_common(10)\n",
    "\n",
    "q3_memory(file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### For the third exercise, we see that in terms of time, the execution time for the q3_time function is lower. In terms of memory, we see that both use approximately 883 KiB of memory; this, like in the previous case, may be due to the fact that both implementations process the file line by line and store it in structures that are already optimized for memory usage. Since neither of the functions loads the file completely, the usage is very similar in that they access the necessary elements in each iteration."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test functions from files"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from q1_time import q1_time\n",
    "from q1_memory import q1_memory\n",
    "from q2_time import q2_time\n",
    "from q2_memory import q2_memory\n",
    "from q3_time import q3_time\n",
    "from q3_memory import q3_memory"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "file_path = \"farmers-protest-tweets-2021-2-4.json\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "[(datetime.date(2021, 2, 12), 'RanbirS00614606'),\n (datetime.date(2021, 2, 13), 'MaanDee08215437'),\n (datetime.date(2021, 2, 17), 'RaaJVinderkaur'),\n (datetime.date(2021, 2, 16), 'jot__b'),\n (datetime.date(2021, 2, 14), 'rebelpacifist'),\n (datetime.date(2021, 2, 18), 'neetuanjle_nitu'),\n (datetime.date(2021, 2, 15), 'jot__b'),\n (datetime.date(2021, 2, 20), 'MangalJ23056160'),\n (datetime.date(2021, 2, 23), 'Surrypuria'),\n (datetime.date(2021, 2, 19), 'Preetm91')]"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1_time(file_path=file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "q1_memory(file_path=file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "q2_time(file_path=file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "q2_memory(file_path=file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "q3_time(file_path=file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "q3_memory(file_path=file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Solution with gcp"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### As an extra exercise, implement a small solution that stores the data in Cloud Storage, a function that stores the data from Cloud Storage in BigQuery, and a test of queries to BigQuery.\n",
    "\n",
    "### The credentials file is not sent, but a service account can be created, keys can be generated, and the path to the keys can be saved as an environment variable called GOOGLE_APPLICATION_CREDENTIALS."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import orjson\n",
    "\n",
    "from google.cloud import storage, bigquery"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Here you can change the file names and the project_id to replicate it."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "file_path = \"farmers-protest-tweets-2021-2-4.json\"\n",
    "bucket_name = \"challenge-latam\"\n",
    "source_file_name = \"corrected_data.json\"\n",
    "destination_blob_name = \"data_farmers_corrected.json\"\n",
    "file_name = \"data_farmers_corrected.json\"\n",
    "project_id = \"latam-challenge-438317\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def fix_source_file(file_path: str):\n",
    "    \"\"\"Function to fix source file and save it as a new file.\"\"\"\n",
    "    objects = []\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        # Read the file line by line\n",
    "        for line in f:\n",
    "            # Fix and convert each line in a JSON object\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                # Ignore empty lines\n",
    "                continue\n",
    "\n",
    "            # Validate the object to process correctly\n",
    "            if line.startswith('{') and line.endswith('}'):\n",
    "                try:\n",
    "                    tweet = orjson.loads(line)\n",
    "                    objects.append(tweet)\n",
    "                except orjson.JSONDecodeError as e:\n",
    "                    print(f'Error decoding the line: {line.strip()} - {e}')\n",
    "\n",
    "        json_string = orjson.dumps(objects)\n",
    "\n",
    "    # Name of new file\n",
    "    file_name = 'corrected_data.json'\n",
    "\n",
    "    # Save JSON as a new file\n",
    "    with open(file_name, 'wb') as json_file:\n",
    "        json_file.write(json_string)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_bucket(bucket_name, storage_client):\n",
    "    \"\"\"Crate bucket in Google Cloud Storage.\"\"\"\n",
    "    # Initialize client\n",
    "    storage_client = storage_client\n",
    "\n",
    "    # Create bucket\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    # Create bucket in GCS\n",
    "    new_bucket = storage_client.create_bucket(bucket)\n",
    "\n",
    "    print(f'Bucket {new_bucket.name} created.')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def upload_to_gcs(bucket_name, source_file_name, destination_blob_name):\n",
    "    \"\"\"Load file to Google Cloud Storage bucket.\"\"\"\n",
    "\n",
    "    # Create client\n",
    "    storage_client = storage.Client.from_service_account_json(os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\"))\n",
    "\n",
    "    # Obtain bucket\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    # Create blob\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    # Load file\n",
    "    try:\n",
    "        blob.upload_from_filename(source_file_name)\n",
    "        print(f\"File {source_file_name} uploaded to {destination_blob_name} in bucket {bucket_name}.\")\n",
    "    except Exception as e:\n",
    "        create_bucket(bucket_name, storage_client)\n",
    "        blob.upload_from_filename(source_file_name)\n",
    "        print(f\"File {source_file_name} uploaded to {destination_blob_name} in bucket {bucket_name}.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def upload_to_bigquery(dataset_id, table_id, rows_to_insert):\n",
    "    \"\"\"Function to upload data to bigquery.\"\"\"\n",
    "    bq_client = bigquery.Client.from_service_account_json(os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\"))\n",
    "    table_ref = bq_client.dataset(dataset_id).table(table_id)\n",
    "    table = bq_client.get_table(table_ref)\n",
    "    errors = bq_client.insert_rows_json(table, rows_to_insert)\n",
    "\n",
    "    if errors:\n",
    "        raise RuntimeError(f\"Error inserting rows: {errors}\")\n",
    "    return f\"Inserted {len(rows_to_insert)} rows into {table_id}.\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def process_json(bucket_name: str, file_name: str):\n",
    "    \"\"\"Function to process JSON and return as a list.\"\"\"\n",
    "    client = storage.Client.from_service_account_json(os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\"))\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(file_name)\n",
    "    json_data = blob.download_as_text()\n",
    "    try:\n",
    "        tweets_data = orjson.loads(json_data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error {e}\")\n",
    "\n",
    "    return tweets_data\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_data_structures(tweets_data):\n",
    "    \"\"\"Function to build data structures to upload to BigQuery.\"\"\"\n",
    "\n",
    "    tweets = []\n",
    "    users = []\n",
    "\n",
    "    for tweet in tweets_data:\n",
    "        user = tweet.get(\"user\")\n",
    "        user_id = user.get(\"id\")  # Obtén el ID del usuario\n",
    "\n",
    "        # Add user to list of users\n",
    "        users.append({\n",
    "            \"username\": user.get(\"username\"),\n",
    "            \"displayname\": user.get(\"displayname\"),\n",
    "            \"id\": user_id,\n",
    "            \"followersCount\": user.get(\"followersCount\"),\n",
    "            \"friendsCount\": user.get(\"friendsCount\"),\n",
    "            \"statusesCount\": user.get(\"statusesCount\"),\n",
    "            \"created\": user.get(\"created\"),\n",
    "        })\n",
    "\n",
    "        # Process and add tweet\n",
    "        tweets.append({\n",
    "            \"url\": tweet.get(\"url\"),\n",
    "            \"date\": tweet.get(\"date\"),\n",
    "            \"content\": tweet.get(\"content\"),\n",
    "            \"id\": tweet.get(\"id\"),\n",
    "            \"replyCount\": tweet.get(\"replyCount\"),\n",
    "            \"retweetCount\": tweet.get(\"retweetCount\"),\n",
    "            \"likeCount\": tweet.get(\"likeCount\"),\n",
    "            \"quoteCount\": tweet.get(\"quoteCount\"),\n",
    "            \"lang\": tweet.get(\"lang\"),\n",
    "            \"source\": tweet.get(\"source\"),\n",
    "            \"user_id\": user_id,\n",
    "            \"mentioned_users\": [user['id'] for user in tweet.get('mentionedUsers', [])] if tweet.get('mentionedUsers') else [],\n",
    "            \"quoted_tweet_id\": tweet.get('quotedTweet', {}).get('id', None) if tweet.get('quotedTweet') else None,\n",
    "            \"quoted_tweet_content\": tweet.get('quotedTweet', {}).get('content', None) if tweet.get('quotedTweet') else None,\n",
    "            \"quoted_tweet_mentioned_users\": [user['id'] for user in tweet.get('quotedTweet', {}).get('mentionedUsers', [])] if tweet.get('quotedTweet') and tweet.get('quotedTweet').get('mentionedUsers', []) else []\n",
    "        })\n",
    "\n",
    "    return tweets, users\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_bigquery_tables(project_id, dataset_id, table_id, schema):\n",
    "    \"\"\"Function to create BigQuery tables.\"\"\"\n",
    "\n",
    "    bq_client = bigquery.Client.from_service_account_json(os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\"))\n",
    "\n",
    "    # ID of new Dataset\n",
    "    dataset_name = f'{project_id}.{dataset_id}'\n",
    "\n",
    "    dataset = bigquery.Dataset(dataset_name)\n",
    "    dataset.description = \"Dataset created to Latam Challenge.\"\n",
    "\n",
    "    # Create Dataset in BigQuery\n",
    "    try:\n",
    "        dataset = bq_client.create_dataset(dataset)\n",
    "        print(f\"Dataset {dataset.dataset_id} successfully created.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating dataset: {e}\")\n",
    "\n",
    "    # Create reference to the Dataset\n",
    "    dataset_ref = bq_client.dataset(dataset_id)\n",
    "\n",
    "    # Define table\n",
    "    table_ref = dataset_ref.table(table_id)\n",
    "\n",
    "    # Create table\n",
    "    table = bigquery.Table(table_ref, schema=schema)\n",
    "\n",
    "    # Create table in BigQuery\n",
    "    try:\n",
    "        table = bq_client.create_table(table)  # Crea la tabla\n",
    "        print(f\"Table {table.table_id} created in dataset {dataset_id}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating table: {e}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def unique_data(tweets, users):\n",
    "    \"\"\"Function to clean data structures.\"\"\"\n",
    "\n",
    "    unique_tweets = []\n",
    "    seen_ids_tweets = set()\n",
    "\n",
    "    for item in tweets:\n",
    "        if item[\"id\"] not in seen_ids_tweets:\n",
    "            unique_tweets.append(item)\n",
    "            seen_ids_tweets.add(item[\"id\"])\n",
    "\n",
    "    unique_users = []\n",
    "    seen_ids_users = set()\n",
    "\n",
    "    for item in users:\n",
    "        if item[\"id\"] not in seen_ids_users:\n",
    "            unique_users.append(item)\n",
    "            seen_ids_users.add(item[\"id\"])\n",
    "\n",
    "    return unique_tweets, unique_users"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### This is the main process. First, the file is corrected and sent to Cloud Storage. Then, the uploaded file is processed; the function downloads the file and returns it as an object. Next, the data structures to be sent to BigQuery are built. The tables and the dataset are created if they do not exist. After that, the data is cleaned, and finally, it is sent to the tables."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fix_source_file(file_path=\"farmers-protest-tweets-2021-2-4.json\")\n",
    "upload_to_gcs(bucket_name, source_file_name, destination_blob_name)\n",
    "tweets_data = process_json(bucket_name=bucket_name, file_name=file_name)\n",
    "tweets, users = build_data_structures(tweets_data=tweets_data)\n",
    "\n",
    "tweets_schema = [\n",
    "    bigquery.SchemaField(\"url\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"date\", \"TIMESTAMP\"),\n",
    "    bigquery.SchemaField(\"content\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"id\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"replyCount\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"retweetCount\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"likeCount\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"quoteCount\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"lang\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"source\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"user_id\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"mentioned_users\", \"INTEGER\", mode=\"REPEATED\"),\n",
    "    bigquery.SchemaField(\"quoted_tweet_id\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"quoted_tweet_content\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"quoted_tweet_mentioned_users\", \"INTEGER\", mode=\"REPEATED\")\n",
    "]\n",
    "\n",
    "users_schema = [\n",
    "    bigquery.SchemaField(\"username\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"displayname\", \"STRING\"),\n",
    "    bigquery.SchemaField(\"id\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"followersCount\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"friendsCount\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"statusesCount\", \"INTEGER\"),\n",
    "    bigquery.SchemaField(\"created\", \"TIMESTAMP\"),\n",
    "]\n",
    "create_bigquery_tables(project_id, \"challenge_data\", \"tweets\", tweets_schema)\n",
    "create_bigquery_tables(project_id, \"challenge_data\", \"users\", users_schema)\n",
    "\n",
    "tweets, users = unique_data(tweets, users)\n",
    "\n",
    "# Insert in BigQuery\n",
    "\n",
    "try:\n",
    "    for elements in range(0, 120000, 10000):\n",
    "        tweet_insert = upload_to_bigquery(\"challenge_data\", \"tweets\", tweets[elements: elements + 10000])\n",
    "    user_insert = upload_to_bigquery(\"challenge_data\", \"users\", users)\n",
    "except Exception as e:\n",
    "    print(f\"Error uploading data to BigQuery: {str(e)}\")\n",
    "\n",
    "print(f\"Upload successful: {tweet_insert}, {user_insert}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Api Implementation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### An API was created with an endpoint to showcase applications that could arise from bringing the data to BigQuery. This API consumes directly from BigQuery, and an endpoint was implemented that solves the first exercise."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Below are images of the test done from Postman, showing the response, and images of the Swagger generated with the API documentation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## In the file 'test from Postman' there is an image of the test conducted on the API that solves exercise 1."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## In the file 'Api Documentation' there is an image of the test conducted on the API that solves exercise 1."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Finally, as potential improvements, GCP logging could be implemented for error handling, which is already in place. Additionally, more extensive error handling could be added, along with unit tests to test the functions or API endpoints."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
