{
 "cells": [
  {
   "cell_type": "raw",
   "source": [
    "En este archivo puedes escribir lo que estimes conveniente. Te recomendamos detallar tu solución y todas las suposiciones que estás considerando. Aquí puedes ejecutar las funciones que definiste en los otros archivos de la carpeta src, medir el tiempo, memoria, etc."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": ""
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Since the file exceeds GitHub's maximum size limit, it has been added to the .gitignore. To run the notebook and the functions, please add the data file to the src folder of the project."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import gc\n",
    "import heapq\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Third-party imports\n",
    "import emoji\n",
    "import memray\n",
    "import orjson\n",
    "import pandas as pd\n",
    "import snakeviz\n",
    "import ujson"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "file_path = \"farmers-protest-tweets-2021-2-4.json\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### First, I tried to load the file in a pandas to evaluate if the file was correct."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_json(file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### As the load fails I checked the file to find what happened. I used an online tool to prove some random objects and I saw that each object was correct. So I assumed that the problem was that the file contained a lot of objects but them were not in a list. In the next cell I read the file and fix it adding each element in a list and convert to a json. I save the file again in a new file."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# List to save the elements\n",
    "json_list = []\n",
    "\n",
    "# Read the file and add each JSON to the list\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        try:\n",
    "            # Load each line as JSON object and append to the list\n",
    "            object_ = json.loads(line)\n",
    "            json_list.append(object_)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f'Error decoding the line: {line.strip()} - {e}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Convert the list to JSON\n",
    "valid_json = json.dumps(json_list, indent=4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open('fixed_data.json', 'w') as end_file:\n",
    "    end_file.write(valid_json)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "file_path_test = \"fixed_data.json\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_json(file_path_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(min(df[\"date\"]), max(df[\"date\"]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### After processing the file like I explain before. I tried to load in a pandas again and this time it was correct. So I was able to visualize the file structure, fields, types and validate the date range of the file as well as the quantity of rows."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### After understanding the problem with the file I solved the first exercise knowing the problems that the file had and knowing that I had to correct it during the execution of the function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### I defined a function to process the file in a way to optimize the execution time. I proved three packages to use the one that had better performance.\n",
    "\n",
    "### I proved with json, ujson and orjson\n",
    "\n",
    "### Finally, after checked the execution time of all of them. I decided to use orjson"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def process_json(file_path):\n",
    "    tweets_per_date = defaultdict(lambda: defaultdict(int))\n",
    "    # Open the file in read mode\n",
    "    with open(file_path, 'r') as f:\n",
    "        # Read the file line by line\n",
    "        for line in f:\n",
    "            # Fix and convert each line in a JSON object\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                # Ignore empty lines\n",
    "                continue\n",
    "\n",
    "            # Validate the object to process correctly\n",
    "            if line.startswith('{') and line.endswith('}'):\n",
    "                try:\n",
    "                    tweet = orjson.loads(line)\n",
    "                except orjson.JSONDecodeError as e:\n",
    "                    print(f'Error decoding the line: {line.strip()} - {e}')\n",
    "\n",
    "                # Obtain date and username of each tweet\n",
    "                tweet_date = datetime.strptime(tweet['date'], '%Y-%m-%dT%H:%M:%S+00:00').date()\n",
    "                username = tweet['user']['username']\n",
    "\n",
    "                # Add to the count of that date and user\n",
    "                tweets_per_date[tweet_date][username] += 1\n",
    "\n",
    "    return tweets_per_date"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Now I create the function to return the top 10 of dates with more tweets and the username with more publications. First, I implemented a loop to extract username and date of each tweet and append to a defaultdict. Here I add a new parameter to the function called \"version\" to proved two versions that I implemented. The first one has a loop over the items of the defaultdict and for each date calculate the total tweets and the user with more tweets, then make a validation with the maximum found. The second one sorted the dates by the total tweets and then just for the top 10 calculate the user with more tweets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def q1_time(file_path: str, version: int) -> List[Tuple[datetime.date, str]]:\n",
    "    tweets_per_date = process_json(file_path)\n",
    "\n",
    "    if version == 1:\n",
    "        top_dates = []\n",
    "        for date, users in tweets_per_date.items():\n",
    "            # Calculate total tweets and find user with more tweets\n",
    "            total_tweets = 0\n",
    "            top_user = None\n",
    "            max_tweets = 0\n",
    "\n",
    "            for user, count in users.items():\n",
    "                total_tweets += count\n",
    "                if count > max_tweets:\n",
    "                    max_tweets = count\n",
    "                    top_user = user\n",
    "\n",
    "            # Add date, total tweets and user to the list\n",
    "            top_dates.append((date, total_tweets, top_user))\n",
    "\n",
    "        # Sort list by total tweets and save just top 10\n",
    "        top_dates_sorted = sorted(top_dates, key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "        # Return date and user in a list of tuples\n",
    "        return [(date, top_user) for date, _, top_user in top_dates_sorted]\n",
    "\n",
    "    else:\n",
    "        # List to save dates and total tweets per date\n",
    "        date_tweet_sums = []\n",
    "        # Dict to save users with more tweets per date\n",
    "        top_users_by_date = {}\n",
    "\n",
    "        # Calculate total tweets and find user with more tweets\n",
    "        for date, users in tweets_per_date.items():\n",
    "            total_tweets = sum(users.values())\n",
    "            top_user = max(users.items(), key=lambda item: item[1])[0]\n",
    "            date_tweet_sums.append((date, total_tweets))\n",
    "            top_users_by_date[date] = top_user\n",
    "\n",
    "        # Sort list by total tweets and save just top 10\n",
    "        top_10_dates = sorted(date_tweet_sums, key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "        # Use the date list and users dict to return the list\n",
    "        return [(tweet_date, top_users_by_date[tweet_date]) for tweet_date, _ in top_10_dates]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Now, for the function optimized by memory use I implemented a function that read the file line by line with a generator. The \"tweet_generator\" function process the file line by line and use yield to return each tweet one by one. With the use of yield avoid that the data are in memory and allow to procees when is necessary. In addition I used heap to implement a priority queue, where the element with the minimum or maximum value can be accessed efficiently, to obtain the k largest or smallest elements from a list efficiently"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def tweet_generator(file_path):\n",
    "    # Open file in read mode\n",
    "    with open(file_path, 'r') as f:\n",
    "        # Read each line\n",
    "        for line in f:\n",
    "            # Delete line breaks and spaces\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            # Validate JSON format\n",
    "            if line.startswith('{') and line.endswith('}'):\n",
    "                # Convert to object\n",
    "                try:\n",
    "                    tweet = json.loads(line)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f'Error decoding the line: {line.strip()} - {e}')\n",
    "\n",
    "                # Save the date and username\n",
    "                tweet_date = datetime.strptime(tweet['date'], '%Y-%m-%dT%H:%M:%S+00:00').date()\n",
    "                username = tweet['user']['username']\n",
    "\n",
    "                # Use of yield to return data one by one\n",
    "                yield tweet_date, username\n",
    "\n",
    "def process_tweets(file_path):\n",
    "    # Dict to count tweets by date and user\n",
    "    tweets_per_date = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    # use generator to process each tweet\n",
    "    for tweet_date, username in tweet_generator(file_path):\n",
    "        # Add to dict\n",
    "        tweets_per_date[tweet_date][username] += 1\n",
    "\n",
    "    return tweets_per_date\n",
    "\n",
    "def q1_memory(file_path):\n",
    "    top_dates_heap = []\n",
    "    tweets = process_tweets(file_path)\n",
    "\n",
    "    # Process file in an incremental way and free memory whenever possible\n",
    "    for date, user_data in tweets.items():\n",
    "        # Total tweets by user\n",
    "        total_tweets = sum(user_data.values())\n",
    "        top_user = max(user_data, key=user_data.get)\n",
    "        top_user_count = user_data[top_user]\n",
    "\n",
    "        # Use heap\n",
    "        heapq.heappush(top_dates_heap, (total_tweets, date, top_user, top_user_count))\n",
    "\n",
    "        # Limit the heap to 10 elements\n",
    "        if len(top_dates_heap) > 10:\n",
    "            heapq.heappop(top_dates_heap)\n",
    "\n",
    "        # Free memory manually\n",
    "        del user_data\n",
    "        gc.collect()\n",
    "\n",
    "    # Convert the heap to sorted list\n",
    "    top_dates_sorted = sorted(top_dates_heap, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    return [(date, top_user) for _, date, top_user, _ in top_dates_sorted]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "q1_time(file_path=file_path, version=2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "q1_memory(file_path=file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Use of snakeviz. SnakeViz is a browser based graphical viewer for the output of Python’s cProfile module. With that package we can see each function and their times"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext snakeviz"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Here I use snakeviz to see the time profile of each function. I run the time function twice, because I was proving the two versions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%snakeviz q1_time(file_path=file_path, version=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%snakeviz q1_time(file_path=file_path, version=2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%snakeviz q1_memory(file_path=file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Use of memray to see the memory used profile"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext memray"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%memray_flamegraph\n",
    "def q1_time(file_path: str, version: int) -> List[Tuple[datetime.date, str]]:\n",
    "    tweets_per_date = process_json(file_path)\n",
    "\n",
    "    if version == 1:\n",
    "        top_dates = []\n",
    "        for date, users in tweets_per_date.items():\n",
    "            # Calculate total tweets and find user with more tweets\n",
    "            total_tweets = 0\n",
    "            top_user = None\n",
    "            max_tweets = 0\n",
    "\n",
    "            for user, count in users.items():\n",
    "                total_tweets += count\n",
    "                if count > max_tweets:\n",
    "                    max_tweets = count\n",
    "                    top_user = user\n",
    "\n",
    "            # Add date, total tweets and user to the list\n",
    "            top_dates.append((date, total_tweets, top_user))\n",
    "\n",
    "        # Sort list by total tweets and save just top 10\n",
    "        top_dates_sorted = sorted(top_dates, key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "        # Return date and user in a list of tuples\n",
    "        return [(date, top_user) for date, _, top_user in top_dates_sorted]\n",
    "\n",
    "    else:\n",
    "        # List to save dates and total tweets per date\n",
    "        date_tweet_sums = []\n",
    "        # Dict to save users with more tweets per date\n",
    "        top_users_by_date = {}\n",
    "\n",
    "        # Calculate total tweets and find user with more tweets\n",
    "        for date, users in tweets_per_date.items():\n",
    "            total_tweets = sum(users.values())\n",
    "            top_user = max(users.items(), key=lambda item: item[1])[0]\n",
    "            date_tweet_sums.append((date, total_tweets))\n",
    "            top_users_by_date[date] = top_user\n",
    "\n",
    "        # Sort list by total tweets and save just top 10\n",
    "        top_10_dates = sorted(date_tweet_sums, key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "        # Use the date list and users dict to return the list\n",
    "        return [(tweet_date, top_users_by_date[tweet_date]) for tweet_date, _ in top_10_dates]\n",
    "\n",
    "q1_time(file_path=file_path, version=2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%memray_flamegraph\n",
    "def tweet_generator(file_path):\n",
    "    # Open file in read mode\n",
    "    with open(file_path, 'r') as f:\n",
    "        # Read each line\n",
    "        for line in f:\n",
    "            # Delete line breaks and spaces\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            # Validate JSON format\n",
    "            if line.startswith('{') and line.endswith('}'):\n",
    "                try:\n",
    "                    # Convert to object\n",
    "                    tweet = json.loads(line)\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f'Error decoding the line: {line.strip()} - {e}')\n",
    "\n",
    "                # Save the date and username\n",
    "                tweet_date = datetime.strptime(tweet['date'], '%Y-%m-%dT%H:%M:%S+00:00').date()\n",
    "                username = tweet['user']['username']\n",
    "\n",
    "                # Use of yield to return data one by one\n",
    "                yield tweet_date, username\n",
    "\n",
    "def process_tweets(file_path):\n",
    "    # Dict to count tweets by date and user\n",
    "    tweets_per_date = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    # use generator to process each tweet\n",
    "    for tweet_date, username in tweet_generator(file_path):\n",
    "        # Add to dict\n",
    "        tweets_per_date[tweet_date][username] += 1\n",
    "\n",
    "    return tweets_per_date\n",
    "\n",
    "def q1_memory(file_path):\n",
    "    top_dates_heap = []\n",
    "    tweets = process_tweets(file_path)\n",
    "\n",
    "    # Process file in an incremental way and free memory whenever possible\n",
    "    for date, user_data in tweets.items():\n",
    "        # Total tweets by user\n",
    "        total_tweets = sum(user_data.values())\n",
    "        top_user = max(user_data, key=user_data.get)\n",
    "        top_user_count = user_data[top_user]\n",
    "\n",
    "        # Use heap\n",
    "        heapq.heappush(top_dates_heap, (total_tweets, date, top_user, top_user_count))\n",
    "\n",
    "        # Limit the heap to 10 elements\n",
    "        if len(top_dates_heap) > 10:\n",
    "            heapq.heappop(top_dates_heap)\n",
    "\n",
    "        # Free memory manually\n",
    "        del user_data\n",
    "        gc.collect()\n",
    "\n",
    "    # Convert the heap to sorted list\n",
    "    top_dates_sorted = sorted(top_dates_heap, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    return [(date, top_user) for _, date, top_user, _ in top_dates_sorted]\n",
    "\n",
    "q1_memory(file_path=file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### After use the packages, we can see that in terms of time the function q1_time is more efficient than q1_memory. However, in terms of memory we can see in the profiling that both functions has the same use of memory that is because in both cases I read the file line by line so just save in memory the last object that I processed. I try to load all the file in memory in q1_time but the orjson.loads function took a lot of time trying to convert the entire file to an object."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### For the second exercise I implemented a function to extract emojis from a text. Then I process the file line by line and extract emojis from the text and save in a Counter.\n",
    "\n",
    "### For this exercise I assumed that I should to find the most used emojis regardless of whether they were in the main tweet or in a quoted tweet.\n",
    "\n",
    "### I used the emoji package because even the use of regular expressions can be more efficient, that way has a complexity in maintain uploaded the list of expressions. The package emoji is uploaded with each new version of unicode"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def extract_emojis(text):\n",
    "    \"\"\"Extract emojis from text.\"\"\"\n",
    "    # Use emoji package to find emojis\n",
    "    return [char for char in text if char in emoji.EMOJI_DATA]\n",
    "\n",
    "def q2_time(file_path):\n",
    "    emoji_counter = Counter()\n",
    "    with open(file_path, 'r') as f:\n",
    "            # Read the file line by line\n",
    "            for line in f:\n",
    "                # Fix and convert each line in a JSON object\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    # Ignore empty lines\n",
    "                    continue\n",
    "\n",
    "                # Validate the object to process correctly\n",
    "                if line.startswith('{') and line.endswith('}'):\n",
    "                    try:\n",
    "                        tweet = orjson.loads(line)\n",
    "                    except orjson.JSONDecodeError as e:\n",
    "                        print(f'Error decoding the line: {line.strip()} - {e}')\n",
    "\n",
    "                # Extract emojis from 'content'\n",
    "                content_emojis = extract_emojis(tweet.get('content', ''))\n",
    "                emoji_counter.update(content_emojis)\n",
    "\n",
    "                # Check if quotedTweet exists and extract emojis from their 'content'\n",
    "                quoted_tweet = tweet.get('quotedTweet')\n",
    "                if quoted_tweet:\n",
    "                    quoted_content_emojis = extract_emojis(quoted_tweet.get('content', ''))\n",
    "                    emoji_counter.update(quoted_content_emojis)\n",
    "\n",
    "            # Obtain top 10 of emojis\n",
    "            top_10 = emoji_counter.most_common(10)\n",
    "\n",
    "            return top_10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "q2_time(file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### For the memory function I used the same approach. I extract emojis with the emoji package from the content of the main tweet and the quoted tweet. And I used generator to process the file in incrementally way"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def extract_emojis(text):\n",
    "    \"\"\"Extract emojis from text.\"\"\"\n",
    "    return [char for char in text if char in emoji.EMOJI_DATA]\n",
    "\n",
    "def tweet_generator(file_path):\n",
    "    \"\"\"Generator to process each tweet and their quotedTweet.\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "            # Read the file line by line\n",
    "            for line in f:\n",
    "                # Fix and convert each line in a JSON object\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    # Ignore empty lines\n",
    "                    continue\n",
    "\n",
    "                # Validate the object to process correctly\n",
    "                if line.startswith('{') and line.endswith('}'):\n",
    "                    try:\n",
    "                        tweet = json.loads(line)\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f'Error decoding the line: {line.strip()} - {e}')\n",
    "\n",
    "                # Process emojis from 'content'\n",
    "                yield extract_emojis(tweet.get('content', ''))\n",
    "\n",
    "                # Process emojis from 'content' in the 'quotedTweet' if it exists\n",
    "                quoted_tweet = tweet.get('quotedTweet')\n",
    "                if quoted_tweet:\n",
    "                    yield extract_emojis(quoted_tweet.get('content', ''))\n",
    "\n",
    "def q2_memory(file_path):\n",
    "    emoji_counter = Counter()\n",
    "\n",
    "    # Use generator to process tweets one by one\n",
    "    for emoji_list in tweet_generator(file_path):\n",
    "        for emoji_char in emoji_list:\n",
    "            emoji_counter[emoji_char] += 1\n",
    "\n",
    "    # Obtain top 10 of emojis\n",
    "    return emoji_counter.most_common(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "q2_memory(file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%snakeviz q2_time(file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%snakeviz q2_memory(file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%memray_flamegraph\n",
    "def extract_emojis(text):\n",
    "    \"\"\"Extract emojis from text.\"\"\"\n",
    "    # Use emoji package to find emojis\n",
    "    return [char for char in text if char in emoji.EMOJI_DATA]\n",
    "\n",
    "def q2_time(file_path):\n",
    "    emoji_counter = Counter()\n",
    "    with open(file_path, 'r') as f:\n",
    "            # Read the file line by line\n",
    "            for line in f:\n",
    "                # Fix and convert each line in a JSON object\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    # Ignore empty lines\n",
    "                    continue\n",
    "\n",
    "                # Validate the object to process correctly\n",
    "                if line.startswith('{') and line.endswith('}'):\n",
    "                    try:\n",
    "                        tweet = orjson.loads(line)\n",
    "                    except orjson.JSONDecodeError as e:\n",
    "                        print(f'Error decoding the line: {line.strip()} - {e}')\n",
    "\n",
    "                # Extract emojis from 'content'\n",
    "                content_emojis = extract_emojis(tweet.get('content', ''))\n",
    "                emoji_counter.update(content_emojis)\n",
    "\n",
    "                # Check if quotedTweet exists and extract emojis from their 'content'\n",
    "                quoted_tweet = tweet.get('quotedTweet')\n",
    "                if quoted_tweet:\n",
    "                    quoted_content_emojis = extract_emojis(quoted_tweet.get('content', ''))\n",
    "                    emoji_counter.update(quoted_content_emojis)\n",
    "\n",
    "            # Obtain top 10 of emojis\n",
    "            top_10 = emoji_counter.most_common(10)\n",
    "\n",
    "            return top_10\n",
    "\n",
    "q2_time(file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%memray_flamegraph\n",
    "def extract_emojis(text):\n",
    "    \"\"\"Extract emojis from text.\"\"\"\n",
    "    return [char for char in text if char in emoji.EMOJI_DATA]\n",
    "\n",
    "def tweet_generator(file_path):\n",
    "    \"\"\"Generator to process each tweet and their quotedTweet.\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "            # Read the file line by line\n",
    "            for line in f:\n",
    "                # Fix and convert each line in a JSON object\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    # Ignore empty lines\n",
    "                    continue\n",
    "\n",
    "                # Validate the object to process correctly\n",
    "                if line.startswith('{') and line.endswith('}'):\n",
    "                    try:\n",
    "                        tweet = json.loads(line)\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f'Error decoding the line: {line.strip()} - {e}')\n",
    "\n",
    "                # Process emojis from 'content'\n",
    "                yield extract_emojis(tweet.get('content', ''))\n",
    "\n",
    "                # Process emojis from 'content' in the 'quotedTweet' if it exists\n",
    "                quoted_tweet = tweet.get('quotedTweet')\n",
    "                if quoted_tweet:\n",
    "                    yield extract_emojis(quoted_tweet.get('content', ''))\n",
    "\n",
    "def q2_memory(file_path):\n",
    "    emoji_counter = Counter()\n",
    "\n",
    "    # Use generator to process tweets one by one\n",
    "    for emoji_list in tweet_generator(file_path):\n",
    "        for emoji_char in emoji_list:\n",
    "            emoji_counter[emoji_char] += 1  # Contar emojis uno por uno\n",
    "\n",
    "    # Obtain top 10 of emojis\n",
    "    return emoji_counter.most_common(10)\n",
    "\n",
    "q2_memory(file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### In the second exercise, after use snakeviz we can see that the function q2_time has a better performance in terms of execution time. For memory, in the profiling we can see that the function q2_memory has a better use of memory but the difference is 2 KiB. In large data sets there could be more difference"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### For exercise 3 like in exercise 2 I assumed that I should to find the most influential users regardless of whether they were in the main tweet or in a quoted tweet."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def q3_time(file_path):\n",
    "    mention_counter = Counter()\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "            # Read the file line by line\n",
    "            for line in f:\n",
    "                # Fix and convert each line in a JSON object\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    # Ignore empty lines\n",
    "                    continue\n",
    "\n",
    "                # Validate the object to process correctly\n",
    "                if line.startswith('{') and line.endswith('}'):\n",
    "                    try:\n",
    "                        tweet = orjson.loads(line)\n",
    "                    except orjson.JSONDecodeError as e:\n",
    "                        print(f'Error decoding the line: {line.strip()} - {e}')\n",
    "\n",
    "                # Process mentions in 'mentionedUsers' from main tweet\n",
    "                mentioned_users = tweet.get('mentionedUsers', [])\n",
    "                if mentioned_users is None:\n",
    "                    mentioned_users = []\n",
    "                mention_counter.update(user['username'] for user in mentioned_users)\n",
    "\n",
    "                # Process mentions in 'mentionedUsers' from 'quotedTweet' if it exists\n",
    "                quoted_tweet = tweet.get('quotedTweet')\n",
    "                if quoted_tweet:\n",
    "                    quoted_mentioned_users = quoted_tweet.get('mentionedUsers', [])\n",
    "                    if quoted_mentioned_users is None:\n",
    "                        quoted_mentioned_users = []\n",
    "                    mention_counter.update(user['username'] for user in quoted_mentioned_users)\n",
    "\n",
    "            # Obtain top 10 of more mentioned users\n",
    "            return mention_counter.most_common(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "q3_time(file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### As well as in the other exercises, for the memory implementation I used generators to maintain the memory with an efficient use."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def mention_generator(file_path):\n",
    "    \"\"\"Generator to process mentions of each tweet and their quotedTweet.\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "            # Read the file line by line\n",
    "            for line in f:\n",
    "                # Fix and convert each line in a JSON object\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    # Ignore empty lines\n",
    "                    continue\n",
    "\n",
    "                # Validate the object to process correctly\n",
    "                if line.startswith('{') and line.endswith('}'):\n",
    "                    try:\n",
    "                        tweet = json.loads(line)\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f'Error decoding the line: {line.strip()} - {e}')\n",
    "\n",
    "                # Process mentions in 'mentionedUsers' from main tweet\n",
    "                mentioned_users = tweet.get('mentionedUsers', [])\n",
    "                if mentioned_users is None:\n",
    "                    mentioned_users = []\n",
    "                for user in mentioned_users:\n",
    "                    yield user['username']\n",
    "\n",
    "                # Process mentions in 'mentionedUsers' from 'quotedTweet' if it exists\n",
    "                quoted_tweet = tweet.get('quotedTweet')\n",
    "                if quoted_tweet:\n",
    "                    quoted_mentioned_users = quoted_tweet.get('mentionedUsers', [])\n",
    "                    if quoted_mentioned_users is None:\n",
    "                        quoted_mentioned_users = []\n",
    "                    for user in quoted_mentioned_users:\n",
    "                        yield user['username']\n",
    "\n",
    "def q3_memory(file_path):\n",
    "    mention_counter = Counter()\n",
    "\n",
    "    # Use generator to process mentions\n",
    "    for username in mention_generator(file_path):\n",
    "        mention_counter[username] += 1\n",
    "\n",
    "    # Obtain top 10 of more mentioned users\n",
    "    return mention_counter.most_common(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "q3_memory(file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%snakeviz q3_time(file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%snakeviz q3_memory(file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%memray_flamegraph\n",
    "def q3_time(file_path):\n",
    "    mention_counter = Counter()\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "            # Read the file line by line\n",
    "            for line in f:\n",
    "                # Fix and convert each line in a JSON object\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    # Ignore empty lines\n",
    "                    continue\n",
    "\n",
    "                # Validate the object to process correctly\n",
    "                if line.startswith('{') and line.endswith('}'):\n",
    "                    try:\n",
    "                        tweet = orjson.loads(line)\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f'Error decoding the line: {line.strip()} - {e}')\n",
    "\n",
    "                # Process mentions in 'mentionedUsers' from main tweet\n",
    "                mentioned_users = tweet.get('mentionedUsers', [])\n",
    "                if mentioned_users is None:\n",
    "                    mentioned_users = []\n",
    "                mention_counter.update(user['username'] for user in mentioned_users)\n",
    "\n",
    "                # Process mentions in 'mentionedUsers' from 'quotedTweet' if it exists\n",
    "                quoted_tweet = tweet.get('quotedTweet')\n",
    "                if quoted_tweet:\n",
    "                    quoted_mentioned_users = quoted_tweet.get('mentionedUsers', [])\n",
    "                    if quoted_mentioned_users is None:\n",
    "                        quoted_mentioned_users = []\n",
    "                    mention_counter.update(user['username'] for user in quoted_mentioned_users)\n",
    "\n",
    "            # Obtain top 10 of more mentioned users\n",
    "            return mention_counter.most_common(10)\n",
    "\n",
    "q3_time(file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%memray_flamegraph\n",
    "def mention_generator(file_path):\n",
    "    \"\"\"Generator to process mentions of each tweet and their quotedTweet.\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "            # Read the file line by line\n",
    "            for line in f:\n",
    "                # Fix and convert each line in a JSON object\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    # Ignore empty lines\n",
    "                    continue\n",
    "\n",
    "                # Validate the object to process correctly\n",
    "                if line.startswith('{') and line.endswith('}'):\n",
    "                    try:\n",
    "                        tweet = json.loads(line)\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f'Error decoding the line: {line.strip()} - {e}')\n",
    "\n",
    "                # Process mentions in 'mentionedUsers' from main tweet\n",
    "                mentioned_users = tweet.get('mentionedUsers', [])\n",
    "                if mentioned_users is None:\n",
    "                    mentioned_users = []\n",
    "                for user in mentioned_users:\n",
    "                    yield user['username']\n",
    "\n",
    "                # Process mentions in 'mentionedUsers' from 'quotedTweet' if it exists\n",
    "                quoted_tweet = tweet.get('quotedTweet')\n",
    "                if quoted_tweet:\n",
    "                    quoted_mentioned_users = quoted_tweet.get('mentionedUsers', [])\n",
    "                    if quoted_mentioned_users is None:\n",
    "                        quoted_mentioned_users = []\n",
    "                    for user in quoted_mentioned_users:\n",
    "                        yield user['username']\n",
    "\n",
    "def q3_memory(file_path):\n",
    "    mention_counter = Counter()\n",
    "\n",
    "    # Use generator to process mentions\n",
    "    for username in mention_generator(file_path):\n",
    "        mention_counter[username] += 1\n",
    "\n",
    "    # Obtain top 10 of more mentioned users\n",
    "    return mention_counter.most_common(10)\n",
    "\n",
    "q3_memory(file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### For the third exercise, we see that in terms of time, the execution time for the q3_time function is lower. In terms of memory, we see that both use approximately 883 KiB of memory; this, like in the previous case, may be due to the fact that both implementations process the file line by line and store it in structures that are already optimized for memory usage. Since neither of the functions loads the file completely, the usage is very similar in that they access the necessary elements in each iteration."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test functions from files"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from q1_time import q1_time\n",
    "from q1_memory import q1_memory\n",
    "from q2_time import q2_time\n",
    "from q2_memory import q2_memory\n",
    "from q3_time import q3_time\n",
    "from q3_memory import q3_memory"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "file_path = \"farmers-protest-tweets-2021-2-4.json\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "q1_time(file_path=file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "q1_memory(file_path=file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "q2_time(file_path=file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "q2_memory(file_path=file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "q3_time(file_path=file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "q3_memory(file_path=file_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
